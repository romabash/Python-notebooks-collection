{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Supervised Learning\n",
    "- Unsupervised learning algorithms learn the structure of datasets\n",
    "- Supervised learning algorithms: label and targets. Estimate a probability distribution $p(y|x)$, by using MLE to find the best parameter vector $\\theta$ for a parametric family of distributions $p(y|x; \\theta)$\n",
    "- Supervised Learning Algorithms:\n",
    "    - Classification\n",
    "    - Regression\n",
    "    - Translation\n",
    "    - Synthesis\n",
    "- Unsupervised Learning Algorithms\n",
    "    - K-mean Classification\n",
    "    - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "- Quantify what it means to do well or poorly on a task by defining a loss function $L(X,Y,\\hat{Y})$, or $L(X,\\hat{Y})$ in unsupervised case, where $\\hat{Y}$ is the output of our neural network\n",
    "- Examples:\n",
    "    - Regression: $\\hat{y}_n(x_n)$ is predicted output: $L = \\Sigma_n || y_n - \\hat{y}_n (x_n) ||^2$\n",
    "    - Classification: y ො n (x n ) is predicted class: $L = \\Sigma_n ( y_n \\neq \\hat{y}_n (x_n) )$\n",
    "    - Clustering: y c is mean of all cases assigned to cluster $c$: $L = \\Sigma_n || x_n - y_c (x_n) ||^2$\n",
    "- Need to find parameters to minimize average loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Log-Likelihood & MSE\n",
    "- Generalize MLE to estimate conditional Probability $P(y|x:\\theta)$\n",
    "- If all inputs and outputs are represented, then:\n",
    "\n",
    "$$ \\theta_{ML} = arg_{\\theta}maxP(Y|X;\\theta) $$\n",
    "\n",
    "$$ \\theta_{ML} = arg_{\\theta}max \\sum_{i=1}^{m} \\log P(y^i|x^i;\\theta)  $$\n",
    "\n",
    "- The most reasonable value for $\\theta$ are those for which probability of the observed sample is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Machine learning algorithm (linear function of the input) where input vector is $x$ and output is $y$, and $\\theta$ is a vector of parameters: $y = \\theta^T x + noise$\n",
    "- Predict $y$ from $x$ based on estimating probability distribution $p(y|x)$.  Use MLE to find the best parameter vector $\\theta$ for a parametric family of distributions\n",
    "\n",
    "$$ p(y|x;\\theta) = Gauss(y;\\theta^Tx,noise) = Gauss(y;\\theta^Tx,\\sigma^2) $$\n",
    "\n",
    "- The log likelihood is the **squared error** cost:\n",
    "\n",
    "$$ L(\\theta,D) = -\\frac{1}{2\\sigma^2}\\sum_m (y^m - \\theta^Tx^m)^2 $$\n",
    "\n",
    "- Compared to the MSE: \n",
    "$$MSE_{train} = \\frac{1}{m} \\sum_{i=1}^m || y^i - \\hat{y}^i ||^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "- The parameters can be solved using the *linear least squares* method, finding the point gradient equal to zero:\n",
    "\n",
    "$$ \\frac{dl}{d\\theta} = -\\sum_m x^m(y^m - | \\theta^T x^m) $$\n",
    "$$ \\theta_{MLE} = (X^TX)^{-1}X^TY $$\n",
    "\n",
    "- There is input correlation matrix and input output correlation matrix, sufficient to represent the statistics of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "- Bayesian approach to linear regression:\n",
    "$$ \\hat{y} = w^Tx $$\n",
    "$$ p(y_{train}|X_{train};w) = Gauss(y_{train};X_{train} w, I) $$\n",
    "$$ \\propto exp(-\\frac{1}{2}(y_{train} - X_{train} w^T) (y_{train} - X_{train} w) $$\n",
    "\n",
    "- To determine posterior, specify prior:\n",
    "$$ p(w) = Gauss(w;\\mu_0,\\lambda_0) $$\n",
    "\n",
    "- Posterior can be written as Gaussian distribution:\n",
    "$$ p(w|X,y) \\propto p(y|X,w)p(w) $$\n",
    "$$ \\propto exp(-\\frac{1}{2}(w - \\mu_m)^T \\lambda_m^{-1} (w - \\mu_m)) $$\n",
    "\n",
    "- Bayesian estimate provides a covariance matrix $\\lambda$ showing how likely all the different values of w are, rather than providing only one estimate for mean value like frequentist approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- For binary variables to define probability distribution, use *logistic regression* approach and model is:\n",
    "$$ p(y=1|x;\\theta) = f(\\theta^Tx) $$\n",
    "\n",
    "- For distribution over a binary variable, its mean must always be between (0,1).  To give mean as parameter, squash the output of the linear function into the interval (0,1) using log sigmoid function and interpret that value as probability\n",
    "- Model for a multinomial random variable whose posterior is the softmax of linear functions of any feature vector:\n",
    "$$ p(y=k|x;\\theta) = \\frac{e^{\\theta^T_k x}}{\\sum_j e^{\\theta^T_j x}} $$\n",
    "\n",
    "- There is no closed form solution for its optimal weights; we find them by maximizing the conditional log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task\n",
    "- Task is to specify which of k categories some input belongs to\n",
    "- Regression approach: algorithm is a function $f: R^n -> \\{1,...,k\\}$\n",
    "- When $y=f(x)$, the model assigns an input described by vector $x$ to a category identified by numeric code $y$\n",
    "- Probabilistic approach: function $f$ outputs probability distribution over classes\n",
    "- There are two approaches generative and discriminative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "- Most popular supervised learning method for classification before NN\n",
    "- Model is similar to logistic regression in that output $y$ is driven by a linear function $w^T x + b$\n",
    "- Unlike logistic regression SVM does not provide probabilities, but only outputs a class identity. The positive class is present when $w^T x + b =1$. Likewise it predicts that negative class is present when $w^T x + b =-1$\n",
    "\n",
    "- The goal of a support vector machine is to find the optimal separating hyperplane which maximizes the margin of the training data, given with equation: $w^T x + b = 0$\n",
    "\n",
    "- Support vectors are critical elements of the training set that would change the position of the dividing hyperplane if removed \n",
    "- Data points that lie closest to the decision surface and are the most difficult to classify\n",
    "- The decision function is specified with subset of training samples: *the Support Vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin\n",
    "- Training vectors : $x_i, i=1, . . . , n$\n",
    "- Feature vectors. For example, A patient = \\[height, weight,...\\].  Consider a simple case with two classes:\n",
    "- Define an indicator vector Y:\n",
    "    - $Y= 1$ if x is in class 1\n",
    "    - $Y= -1$ if x is in class 2\n",
    "- The distance form a point $(x_0 , y_0)$, to a line $Ax + By +c=0$ is:\n",
    "$$ \\frac{|Ax+By+c|}{\\sqrt{(A^2+B^2)}} $$\n",
    "so the margin is \n",
    "$$ 2\\frac{|wx+b|}{||w||} = \\frac{2}{||w||} $$\n",
    "\n",
    "- In order to maximize the margin we need to minimize the $w$. With the condition that there are no data points in the channel:\n",
    "$$ y_i(x_i w) \\geq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint optimization problem\n",
    "- Problem to minimize $||w||$ , rewrite as: $ min f = \\frac{1}{||w^2||}$\n",
    "- This is a quadratic function with constraint: $y_i(x_i w + b) − 1 = 0$\n",
    "- Using Lagrange method solution has to satisfy:\n",
    "$$ w = \\sum_{i=1}^l a_iy_ix_i,\\space \\sum_{i=1}^l a_iy_i = 0$$\n",
    "$$ maxL_D(a_i) = \\sum_{i=1}^l  a_i - \\frac{1}{2} \\sum_{i=1}^l a_ia_jy_iy_j(x_i \\cdot x_j)$$\n",
    "$$ s.t. \\sum_{i=1}^l a_iy_i = 0 \\space \\& \\space a_i \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation plane\n",
    "- After calculating: \n",
    "$$ w = \\sum_{i=1}^l a_iy_ix_i $$\n",
    "- Given an unknown point u measured on features x i we can classify it by looking at the sign of:\n",
    "$$ f(x) = w \\cdot u + b = (\\sum_{i=1}^l a_iy_ix_i \\cdot u) +b $$\n",
    "- Most of the weights $w_i$ will be zero.  Only the support vectors (on the gutters or margin) will have nonzero weights or a’s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVM\n",
    "- The idea is to gain linearly separation by mapping the data to a higher dimensional space \n",
    "- Map to gain linear separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Method\n",
    "- Write a dot product of all examples so SVM algorithm can be written as:\n",
    "$$ w^T x + b = \\sum_{i=1}^l a_i y^i x^i x + b $$\n",
    "- Replace $x$ with the output of a given feature function $\\phi(x)$ and the dot product with a function **Kernel**:\n",
    "$$ k(x, x^i) = \\phi (x) \\phi(x^i)$$\n",
    "- After replacing dot product with kernel evaluations, make a prediction using the function:\n",
    "$$ f(x) = b + \\sum_{i=1}^m a_ik(x,x^i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Kernel Functions\n",
    "$$ f(x) = b + \\sum_{i=1}^m a_ik(x,x^i) $$\n",
    "- This function is nonlinear with respect to $x$, but the relationship between $\\phi(x)$ and $f(x)$ is linear. The kernel trick is equivalent to preprocessing the data by applying $\\phi(x)$ to all inputs, then learning a linear model in the new transformed space\n",
    "- Popular choices for kernel in the SVM literature are:\n",
    "$$ D^{th}-Degree \\space Polynomial: k(x,x^i) = (1+(x,x^i))^d $$\n",
    "$$ Radial: k(x,x^i) = exp(-y ||x-x^i||^2) $$\n",
    "$$ Neural \\space Network: k(x,x^i) = tanh(k_1(x,x^i)+k_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No linear separation possible\n",
    "- Map data into a richer feature space including nonlinear features, then construct a hyperplane in that space so all other equations are the same!\n",
    "$$ x -> \\phi(x) $$\n",
    "- And then learn the map from $\\phi(x)$ to $y$:\n",
    "$$ f(x) = w^T \\cdot \\phi(x) +b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "- Task is to find the *best* representation of the data. It preserves as much information about $x$ as possible while obeying some penalty or constraint aimed at keeping the representation simpler or more accessible than $x$ itself\n",
    "- Simpler representation can be defined by 3 criteria:\n",
    "    - Lower dimensional representation\n",
    "    - Sparse representation\n",
    "    - Independent representation\n",
    "- We will learn example of simple representation learning algorithm **PCA** based on criteria above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "- The goal of this algorithm is to find groups in the data\n",
    "- The algorithm works iteratively to assign each data point to one of *K* groups based on the features that are provided.\n",
    "- Data points are clustered based on feature similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering - Example\n",
    "1. Select how many clusters to identify\n",
    "2. Randomly guess *k* cluster Center locations\n",
    "3. Each data point finds out which Center it’s closest to.\n",
    "4. Each Center finds the centroid of the points it owns\n",
    "5. ...and jumps there\n",
    "6. ...Repeat until terminated\n",
    "- $c_i$ is the collection of centroids in set $C$, then each data point $x$ is assigned to a cluster based on:\n",
    "$$ argmin \\space dist_{c_i}(c_i,x)^2$$\n",
    "\n",
    "where $dist( \\cdot )$ is the standard $(L_2)$ Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find good optimum\n",
    "- Be careful about where you start\n",
    "- Do many runs of k-means, each from a different random start configuration\n",
    "- Many variations of the method exist today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Use Cases\n",
    "- Behavioral segmentation:\n",
    "    - Segment by purchase history\n",
    "    - Define personas based on interests\n",
    "    - Create profiles based on activity monitoring\n",
    "- Inventory categorization:\n",
    "    - Group inventory by sales activity\n",
    "- Sorting measurements:\n",
    "    - Detect activity types in motion sensors\n",
    "    - Group images\n",
    "    - Separate audio\n",
    "    - Identify groups in health monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis: PCA \n",
    "- PCA algorithm provides means of compressing thedata; used in face recognition and image compression\n",
    "- It learns representation of data that has lower dimensionality than the original input\n",
    "- Collection of $m$ points $\\{x_1, x_2, ... x_m\\}$ in space $R^n$ find a corresponding code vector $z_i$ in the space $R^l,l<n$\n",
    "- If $X$ is the design matrix and data has mean of zero $E(x) = 0$ then the sample covariance matrix associated with $X$ is given by $Var[x] = \\frac{1}{m-1}X^TX$\n",
    "- PCA finds a representation through a linear transformation $z = w^Tx$ where $Var[z]$ is diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of PCA\n",
    "- PCA is an unsupervised learning algorithm that learns lower dimensional representation of data\n",
    "- Disentangles the unknown factors of variation underlying the data by rotating the input space such that it aligns the principal axes of variance with the basis of the new representation space\n",
    "- Identifying and removing redundancy enables the dimensionality reduction algorithm to achieve more compression\n",
    "- PCA transforms data into new representation where the elements are mutually uncorrelated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
