{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Natural Language Processing:\n",
    "- Represent total *vocabulary* as an ordered array *dictionary*\n",
    "- Represent each word in the *dictionary* as **one-hot encoded** vector based on the position on that word in the *dictionary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Unidirectional RNN Model:\n",
    "- Represent input and output as a sequence: $x^{<1>}, x^{<2>}, ... , x^{<t>} \\rightarrow y^{<1>}, y^{<2>}, ... , y^{<t>} $\n",
    "- Length on input sequence is denoted as $T_x$, and output sequence as $T_y$\n",
    "- Feed each input into the network to predict $\\hat{y}$.  Information from previous inputs gets passed into the current input as activation.\n",
    "$$a_{<0>} \\rightarrow \\downarrow x^{<1>} \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\end{bmatrix} \\uparrow \\hat{y}^{<1>} \\rightarrow a_{<1>} \\rightarrow \\downarrow x^{<2>} \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\end{bmatrix} \\uparrow \\hat{y}^{<2>} \\rightarrow ... a_{<T_x -1>} \\rightarrow \\downarrow x^{<T_x>} \\begin{bmatrix} 0\\\\0\\\\0\\\\0\\end{bmatrix} \\uparrow \\hat{y}^{<T_Y>}$$\n",
    "\n",
    "- $x^{<1>}$ and $a^{<0>}$ are used to calculate $\\hat{y}^{<1>}$ \n",
    "- $x^{<2>}$ and $a^{<1>}$ are used to calculate $\\hat{y}^{<2>}$ \n",
    "- $x^{<T_x>}$ and $a^{<T_x-1>}$ are used to calculate $\\hat{y}^{<T_y>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "$$ a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) $$\n",
    "  \n",
    "$$ \\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y) $$\n",
    "\n",
    "## Backpropagation:\n",
    "- Croos Entropy Loss Function: \n",
    "\n",
    "$$\\sum_{t=1}^{T_y} L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$$\n",
    "where:\n",
    "$$ L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = -y^{<t>}\\log \\hat{y}^{<t>} - (1-y^{<t>}) \\log (1-\\hat{y}^{<t>}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of RNN Architectures:\n",
    "- One-to-One (Generic NN)\n",
    "- One-to-Many (Generation, ex. Music generation)\n",
    "    - One input with many outputs, where each output is fed back into the network as an *input*\n",
    "- Many-to-One (Classification)\n",
    "- Many-to-Many \n",
    "    - Same length for Input and Output\n",
    "    - Different length for Input and Output (Language Translation).  Encoder and Decoder parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
