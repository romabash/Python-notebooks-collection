{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Supervised Learning\n",
    "- Unsupervised learning algorithms learn the structure of datasets\n",
    "- Supervised learning algorithms: label and targets. Estimate a probability distribution $p(y|x)$, by using MLE to find the best parameter vector $\\theta$ for a parametric family of distributions $p(y|x; \\theta)$\n",
    "- Supervised Learning Algorithms:\n",
    "    - Classification\n",
    "    - Regression\n",
    "    - Translation\n",
    "    - Synthesis\n",
    "- Unsupervised Learning Algorithms\n",
    "    - K-mean Classification\n",
    "    - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "- Quantify what it means to do well or poorly on a task by defining a loss function $L(X,Y,\\hat{Y})$, or $L(X,\\hat{Y})$ in unsupervised case, where $\\hat{Y}$ is the output of our neural network\n",
    "- Examples:\n",
    "    - Regression: $\\hat{y}_n(x_n)$ is predicted output: $L = \\Sigma_n || y_n - \\hat{y}_n (x_n) ||^2$\n",
    "    - Classification: y ො n (x n ) is predicted class: $L = \\Sigma_n ( y_n \\neq \\hat{y}_n (x_n) )$\n",
    "    - Clustering: y c is mean of all cases assigned to cluster $c$: $L = \\Sigma_n || x_n - y_c (x_n) ||^2$\n",
    "- Need to find parameters to minimize average loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Log-Likelihood & MSE\n",
    "- Generalize MLE to estimate conditional Probability $P(y|x:\\theta)$\n",
    "- If all inputs and outputs are represented, then:\n",
    "\n",
    "$$ \\theta_{ML} = arg_{\\theta}maxP(Y|X;\\theta) $$\n",
    "\n",
    "$$ \\theta_{ML} = arg_{\\theta}max \\sum_{i=1}^{m} \\log P(y^i|x^i;\\theta)  $$\n",
    "\n",
    "- The most reasonable value for $\\theta$ are those for which probability of the observed sample is largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Machine learning algorithm (linear function of the input) where input vector is $x$ and output is $y$, and $\\theta$ is a vector of parameters: $y = \\theta^T x + noise$\n",
    "- Predict $y$ from $x$ based on estimating probability distribution $p(y|x)$.  Use MLE to find the best parameter vector $\\theta$ for a parametric family of distributions\n",
    "\n",
    "$$ p(y|x;\\theta) = Gauss(y;\\theta^Tx,noise) = Gauss(y;\\theta^Tx,\\sigma^2) $$\n",
    "\n",
    "- The log likelihood is the **squared error** cost:\n",
    "\n",
    "$$ L(\\theta,D) = -\\frac{1}{2\\sigma^2}\\sum_m (y^m - \\theta^Tx^m)^2 $$\n",
    "\n",
    "- Compared to the MSE: \n",
    "$$MSE_{train} = \\frac{1}{m} \\sum_{i=1}^m || y^i - \\hat{y}^i ||^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "- The parameters can be solved using the *linear least squares* method, finding the point gradient equal to zero:\n",
    "\n",
    "$$ \\frac{dl}{d\\theta} = -\\sum_m x^m(y^m - | \\theta^T x^m) $$\n",
    "$$ \\theta_{MLE} = (X^TX)^{-1}X^TY $$\n",
    "\n",
    "- There is input correlation matrix and input output correlation matrix, sufficient to represent the statistics of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression\n",
    "- Bayesian approach to linear regression:\n",
    "$$ \\hat{y} = w^Tx $$\n",
    "$$ p(y_{train}|X_{train};w) = Gauss(y_{train};X_{train} w, I) $$\n",
    "$$ \\propto exp(-\\frac{1}{2}(y_{train} - X_{train} w^T) (y_{train} - X_{train} w) $$\n",
    "\n",
    "- To determine posterior, specify prior:\n",
    "$$ p(w) = Gauss(w;\\mu_0,\\lambda_0) $$\n",
    "\n",
    "- Posterior can be written as Gaussian distribution:\n",
    "$$ p(w|X,y) \\propto p(y|X,w)p(w) $$\n",
    "$$ \\propto exp(-\\frac{1}{2}(w - \\mu_m)^T \\lambda_m^{-1} (w - \\mu_m)) $$\n",
    "\n",
    "- Bayesian estimate provides a covariance matrix $\\lambda$ showing how likely all the different values of w are, rather than providing only one estimate for mean value like frequentist approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- For binary variables to define probability distribution, use *logistic regression* approach and model is:\n",
    "$$ p(y=1|x;\\theta) = f(\\theta^Tx) $$\n",
    "\n",
    "- For distribution over a binary variable, its mean must always be between (0,1).  To give mean as parameter, squash the output of the linear function into the interval (0,1) using log sigmoid function and interpret that value as probability\n",
    "- Model for a multinomial random variable whose posterior is the softmax of linear functions of any feature vector:\n",
    "$$ p(y=k|x;\\theta) = \\frac{e^{\\theta^T_k x}}{\\sum_j e^{\\theta^T_j x}} $$\n",
    "\n",
    "- There is no closed form solution for its optimal weights; we find them by maximizing the conditional log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task\n",
    "- Task is to specify which of k categories some input belongs to\n",
    "- Regression approach: algorithm is a function $f: R^n -> \\{1,...,k\\}$\n",
    "- When $y=f(x)$, the model assigns an input described by vector $x$ to a category identified by numeric code $y$\n",
    "- Probabilistic approach: function $f$ outputs probability distribution over classes\n",
    "- There are two approaches generative and discriminative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "- Most popular supervised learning method for classification before NN\n",
    "- Model is similar to logistic regression in that output $y$ is driven by a linear function $w^T x + b$\n",
    "- Unlike logistic regression SVM does not provide probabilities, but only outputs a class identity. The positive class is present when $w^T x + b =1$. Likewise it predicts that negative class is present when $w^T x + b =-1$\n",
    "\n",
    "- The goal of a support vector machine is to find the optimal separating hyperplane which maximizes the margin of the training data, given with equation: $w^T x + b = 0$\n",
    "\n",
    "- Support vectors are critical elements of the training set that would change the position of the dividing hyperplane if removed \n",
    "- Data points that lie closest to the decision surface and are the most difficult to classify\n",
    "- The decision function is specified with subset of training samples: *the Support Vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin\n",
    "- Training vectors : $x_i, i=1, . . . , n$\n",
    "- Feature vectors. For example, A patient = \\[height, weight,...\\].  Consider a simple case with two classes:\n",
    "- Define an indicator vector Y:\n",
    "    - $Y= 1$ if x is in class 1\n",
    "    - $Y= -1$ if x is in class 2\n",
    "- The distance form a point $(x_0 , y_0)$, to a line $Ax + By +c=0$ is:\n",
    "$$ \\frac{|Ax+By+c|}{\\sqrt{(A^2+B^2)}} $$\n",
    "so the margin is \n",
    "$$ 2\\frac{|wx+b|}{||w||} = \\frac{2}{||w||} $$\n",
    "\n",
    "- In order to maximize the margin we need to minimize the $w$. With the condition that there are no data points in the channel:\n",
    "$$ y_i(x_i w) \\geq 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint optimization problem\n",
    "- Problem to minimize $||w||$ , rewrite as: $ min f = \\frac{1}{||w^2||}$\n",
    "- This is a quadratic function with constraint: $y_i(x_i w + b) − 1 = 0$\n",
    "- Using Lagrange method solution has to satisfy:\n",
    "$$ w = \\sum_{i=1}^l a_iy_ix_i,\\space \\sum_{i=1}^l a_iy_i = 0$$\n",
    "$$ maxL_D(a_i) = \\sum_{i=1}^l  a_i - \\frac{1}{2} \\sum_{i=1}^l a_ia_jy_iy_j(x_i \\cdot x_j)$$\n",
    "$$ s.t. \\sum_{i=1}^l a_iy_i = 0 \\space \\& \\space a_i \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation plane\n",
    "- After calculating: \n",
    "$$ w = \\sum_{i=1}^l a_iy_ix_i $$\n",
    "- Given an unknown point u measured on features x i we can classify it by looking at the sign of:\n",
    "$$ f(x) = w \\cdot u + b = (\\sum_{i=1}^l a_iy_ix_i \\cdot u) +b $$\n",
    "- Most of the weights $w_i$ will be zero.  Only the support vectors (on the gutters or margin) will have nonzero weights or a’s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
