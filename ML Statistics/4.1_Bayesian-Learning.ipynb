{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior and Posterior Probability\n",
    "- Two different approaches in statistics: Bayesianism and Frequentism\n",
    "- Bayesian statistics modeling is trying to encode initial beliefs and to update the belief as we observe the new data.  Useful when data is limited.  Assign probability to hypothesis based on belief.\n",
    "- Frequentist statistics computes probability of an event that occurs many many times, with no prior beliefs\n",
    "- Prior: Previous beliefs about an object before it has observed $x$\n",
    "- Posterior: Probability that is computed after observation of $x$\n",
    "- Stochastic event – A random varibale $x$ is function of both time and outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baye’s Rule\n",
    "- Baye’s Rule: inference about hypothesis from data\n",
    "- Hypothesis space is set of all concepts or functions that learning algorithm is allowed to select as being the solution to the problem: “coin is fair”...\n",
    "- When making prediction for new data, consider effect of measured data on hypothesis about data:\n",
    "\n",
    "$$ P(Hypothesis|Data) = \\frac{P(Data|Hypothesis)P(Hypoyhesis)}{P(Data)} $$\n",
    "where $P(Hypothesis)$ is a **Prior** and $P(Hypothesis|Data)$ is **Posterior Prediction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior and Parametric Models\n",
    "- Prior in Bayesian statistics is $P(hypothesis)$ where hypothesis is controlled with parameters $\\theta$,(vector $\\theta$)\n",
    "- Parametric models in ML assume some finite set of parameters $\\theta$ that capture everything there is to know\n",
    "about the data \n",
    "\n",
    "$$P(X|\\theta, {x_1, . . x_m}) = P(X|\\theta) = P(Data|Hypothesis)$$\n",
    "\n",
    "- Goal is to find the posterior predictive distribution $P(Hypothesis|Data)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "- What if there are multiple hypothesis consistent with evidence, (multiple priors)\n",
    "- Definition of Likelihood is $p(X|\\theta)$\n",
    "- Example: $X=[16,8,2,64]$ given two hypothesis: “power of two” and “even numbers” \n",
    "\n",
    "$$p(X|\\theta) = \\left(\\frac{1}{size(hypothesis}\\right)^n = \\left(\\frac{1}{|\\theta|}\\right)^n$$ \n",
    "where $n$ is number of samples\n",
    "\n",
    "- Models favor simplest hypothesis consistent with the data, (Occam’s razor principle)\n",
    "    - \"Power of two\" is the hypothesis that covers this situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Models\n",
    "- Parametric models assume some finite set of parameters $\\theta$. Given the parameters, future predictions, $x$, are independent of the observed data, $X$:\n",
    "\n",
    "$$P(x|\\theta, X, m) = P(x|\\theta, m)$$\n",
    "\n",
    "- Therefore $\\theta$ capture everything there is to know about the data.\n",
    "- So the complexity of the model is bounded even if the amount of data is unbounded. This **does not** makes them very flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Machine Learning\n",
    "- $P(\\theta, m)$ is prior probability of $\\theta$\n",
    "- $P(X|\\theta, m)$ is likelihood of parameters $\\theta$ in model $m$\n",
    "- Posterior probability of $\\theta$ given data $X$ is:\n",
    "\n",
    "$$ P(\\theta|X,m) = \\frac{P(X|\\theta,m)P(\\theta,m)}{P(X|m)}$$\n",
    "\n",
    "- Prediction:\n",
    "\n",
    "$$P(x|X,m) = \\int P(x|\\theta,X,m)P(\\theta|X,m)d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example...\n",
    "- Suppose that we have two bags each containing black and white balls.\n",
    "- One bag contains three times as many white balls as blacks. The other bag contains three times as many black balls as white.\n",
    "- Suppose we choose one of these bags at random. For this bag we select five balls at random, replacing each ball after it has been selected. The result is that we find 4 white balls and one black.\n",
    "- What is the probability that we were using the bag with mainly white balls?\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{\\Sigma_i P(B|A_i)P(A_i)} $$\n",
    "where\n",
    "$$ P(B) = \\Sigma_i P(B, A_i) = \\Sigma_i P(B|A_i)P(A_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution \n",
    "- Let $A$ be the random variable \"bag chosen\" then $A=[a1,a2]$ where $a1$ represents \"bag with mostly white balls\" and $a2$ represents \"bag with mostly black balls\". \n",
    "- We know that $P(a1)=P(a2)=1/2$ since we choose the bag at random.\n",
    "- Let $B$ be the event \"4 white balls and one black ball chosen from 5 selections\".\n",
    "- Now, for the bag with mostly white balls the probability of a ball being white is 3⁄4 and the probability of a ball being black is 1⁄4. Thus, we can use the **Binomial Theorem**, to compute $P(B|a1) and P(B|a2)$ as:\n",
    "\n",
    "$$ P(B|a_1) = \\left(_{1}^{5}\\right) \\left(\\frac{3}{4}\\right)^4 \\left(\\frac{1}{4}\\right)^1 = \\frac{405}{1024} $$\n",
    "\n",
    "$$ P(B|a_2) = \\left(_{1}^{5}\\right) \\left(\\frac{1}{4}\\right)^4 \\left(\\frac{3}{4}\\right)^1 = \\frac{15}{1024} $$\n",
    "\n",
    "- Then calculate $P(a_1|B)$ from Baye's rule:\n",
    "\n",
    "$$ P(a_1|B) = \\frac{\\frac{405}{1024}}{\\frac{405}{1024}+\\frac{15}{1024}} = \\frac{405}{420} = 0.964 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Statistics\n",
    "- In general before observing the data we represent our knowledge of $\\theta$ using the prior probability distribution $p(\\theta)$ with high entropy, as high degree of uncertainty\n",
    "- For a set of data samples $X =$ {$x^{(1)} , x^{(2)}, ..., x^{(m)}$} consider their effect on hypothesis about $\\theta$ by combining the data likelihood $p(x^{(1)} , x^{(2)}, ..., x^{(m)})|\\theta)$ with the prior Bayes rule:\n",
    "\n",
    "$$ p(\\theta|x^{(1)} , x^{(2)}, ..., x^{(m)}) = \\frac{p(x^{(1)} , x^{(2)}, ..., x^{(m)}|\\theta)p(\\theta)}{p(x^{(1)} , x^{(2)}, ..., x^{(m)})} $$\n",
    "\n",
    "- After observing $m$ examples the predicted distribution over the next data sample $x^{(m+1)}$ is:\n",
    "\n",
    "$$ p(x^{(m+1)} | x^{(1)}, x^{(2)}, ...,x^{(m)} )= \\int p(x^{(m+1)}|\\theta) p(\\theta| x ^{(1)} , x^{(2)}, ...,x^{(m)})dθ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max A-Posteriori Estimation\n",
    "- The MAP estimate chooses the point of maximal posterior probability or maximal probability density in the more common case of continuous $\\theta$ \n",
    "\n",
    "$$\\theta_{MAP} = arg_{\\theta} max p(\\theta|x) = arg_{\\theta}max \\log p(x|\\theta) + \\log p(\\theta)$$\n",
    "\n",
    "- Here $log p(x|\\theta)$ is the standard likelihood term and $log p(\\theta)$ corresponds to the prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Models\n",
    "- They incorporate random variables and probabilistic distributions as outcome\n",
    "- Faithfully represent uncertainty in model structure and parameters and noise in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM)\n",
    "- Mixture of distributions: $P(x) = \\sum_i P(c=i)P(x|c=i)$ , where $P(c)$ is multinoulli distribution over component identities\n",
    "- In Gaussian Mixture Model the components $p(x|c=i)$ are Gaussians, each component has separately parametrized mean $\\mu^{(i)}$ , variance $\\sigma^{(i)}$ and covariance ($cap \\space \\sigma$) $\\sum(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Graphical Model\n",
    "- Probabilistic graphical models represent large joint distributions compactly using a set of “local” relationships specified by a graph.\n",
    "- Each random variable in our model corresponds to a graph node.\n",
    "- There are directed/undirected edges between the nodes which tell us qualitatively about the factorization of the joint probability.\n",
    "- There are functions stored at the nodes which tell us the quantitative details of the pieces into which the distribution factors\n",
    "- Graphical models are also known as Bayes(ian) (Belief) Net(work)s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Graphical Model\n",
    "- Consider directed acyclic graphs over $n$ variables.\n",
    "- Each node has set of parents $\\pi_i$\n",
    "- Each node maintains a function $f_i (x_i;X_{\\pi i})$ such that\n",
    "$$ f_i > 0 \\space and \\space \\Sigma_{xi} f_i(x_i;X_{\\pi i}) =1 $$\n",
    "- Define the joint probability to be\n",
    "$$ P(x_1, x_2,...,x_n) = \\prod_i P(x_i|X_{\\pi i}) $$\n",
    "- Factorization of the joint in terms of local conditional probabilities. Exponential in “fan-in” of each node instead of in total variables $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Graphical Model\n",
    "- Graphical models aim to provide compact factorizations of large joint probability distributions.\n",
    "- These factorizations are achieved using local functions which exploit conditional independencies in the models.\n",
    "- The graph tells us a basic set of conditional independencies that must be true.\n",
    "- These independencies are crucial to developing efficient algorithms valid for all numerical settings of the local functions.\n",
    "- Local functions tell us the quantitative details of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Given examples of a discrete class label y and some features $x$.\n",
    "- Goal: compute label $(y)$ for new inputs $x$.\n",
    "- Two approaches:\n",
    "    - Generative: model $p(x, y) = p(y)p(x|y)$; use Bayes’ rule to infer conditional $p(y|x)$.\n",
    "    - Discriminative: model discriminants $f(y|x)$ directly and take max.\n",
    "    - Generative approach is related to conditional density estimation while discriminative approach is closer to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Classification: Bayes Classifiers\n",
    "- Generative model: $p(x, y) = p(y)p(x|y)$\n",
    "    - $p(y)$ are called class priors.\n",
    "    - $p(x|y)$ are called class conditional feature distributions.\n",
    "- For the prior we use a Bernoulli or multinomial:\n",
    "- $p(y = k|\\pi) = \\pi_i \\space with \\space \\Sigma_k \\pi_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifiers\n",
    "- Classification rules:\n",
    "    - ML: $argmax_y p(x|y)$ (can behave badly if skewed priors)\n",
    "    - MAP: $argmax_y p(y|x)$ = $argmax_y(\\log p(x|y) + \\log p(y))$ (safer)\n",
    "- Fitting:\n",
    "    1. Sort data into batches by class label.\n",
    "    2. Estimate $p(y)$ by counting size of batches (plus regularization).\n",
    "    3. Estimate $p(x|y)$ separately within each batch using ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Key Regularization Ideas\n",
    "- To avoid overfitting, put priors on the parameters of the class and class conditional feature distributions.\n",
    "- Tie some parameters together so that fewer of them are estimated using more data.\n",
    "- Make factorization or independence assumptions about the distributions. In particular, for the class conditional distributions assume the features are fully dependent, partly dependent, or independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "- Assumption: conditioned on class, attributes are independent. $P(x_1,x_2,x_n|y) = \\prod_i P(x_i|X_{\\pi i})$\n",
    "- Algorithm: sort data cases into bins according to $y_n$ . Compute marginal probabilities $p(y = c)$ using frequencies.\n",
    "- For each class, estimate distribution of $i^th$ variable: $p(x_i |y = c)$.\n",
    "- At test time, compute $argmax_c p(c|x)$ using $c(x) = argmax_c p(c|x) = argmax_c [\\log p(x|c) + \\log p(c)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
