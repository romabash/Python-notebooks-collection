{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perceptrons:\n",
    "- Perceptrons produce binary output based on the binary input $X: \\{x_1, x_2, x_3, ...x_n\\}$ and the weights $W: \\{w_1, w_2, w_3, ...w_n\\}$associated with each input.  \n",
    "- Neuron's output (0 or 1) is determined by whether the weighted sum $\\sum_i w_ix_i$ is less than or greater than some *threshold value* (adjustable real number parameter)\n",
    "- Perceptron with a large *bias* will output 1 easier than a Perceptron with a really small *bias*\n",
    "- Multi Level Perceptrons (MLP) make decision by weighing up the results from the previous layer.\n",
    "- Perceptron decision making can be summarized as:\n",
    "\n",
    "$$ output  =\n",
    "    \\begin{cases}\n",
    "      0 & w \\cdot x + b \\leq 0\\\\\n",
    "      1 & w \\cdot x + b \\leq 0\\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "where $w \\cdot x  \\equiv \\sum_i w_ix_i$ and *bias* $b \\equiv $ *-threshold*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neuron: \n",
    "- One problems with Perceptrons is that in order to learn, small change in a *weight* or *bias* should cause only a small change in output, but in the Perceptron, small changes can cause the output to completely change from 0 to 1, cnaging the behavior of the rest of the Network\n",
    "- Unlike binary inputs and outputs in the Perceptron Neural Network, in Sigmoid Network the inputs and weights can be any real value between 0 and 1\n",
    "- The output is determined by the *sigmoid function* $\\sigma(w \\cdot x + b)$ where:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-(\\sum_i w_ix_i + b)}} $$\n",
    "\n",
    "- If $z \\equiv w \\cdot x + b$ then when $z$ is a very large number, $e^{-z} \\approx 0$ and the output $\\sigma(z) \\approx 1$\n",
    "- When $z$ is a very small number, $e^{-z} \\rightarrow \\infty$ and the output $\\sigma(z) \\approx 0$\n",
    "- Sigmois differs from Perceptron when $z$ is a modest number\n",
    "- The smoothness of a Sigmois function means that small changes in *weights* and *bias* produce small changes in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions:\n",
    "- Other Activation function besides Sigmod include *ReLU* (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
