{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting:\n",
    "- Use regularization techniques to reduce overfitting\n",
    "- Increase *training data* to reduce overfitting\n",
    "- Early Stopping: track of accuracy on the *validation data* as the network trains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization:\n",
    "- Adds an extra term to the Cost Function as a weight decay to make the network learn small weights:\n",
    "\n",
    "$$ C = C_0 + \\frac{\\lambda}{2n} \\sum_w w^2 $$\n",
    "<div style=\"text-align: center; font-size: 10px\"> where $C_0$ is the original Cost Function </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization:\n",
    "-  The sum of the squares of all the weights scaled by a factor $ \\frac{\\lambda}{2n}$, where $\\lambda>0$ is the regularization parameter:\n",
    "- Small $\\lambda$ tends to minimize the *Cost function*, large $\\lambda$ tends to prefer small weights\n",
    "- The partial derivatives of the *Cost function* (claculated using backpropagation) becomes:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial C}{\\partial w}  =  \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\\\\n",
    "\\frac{\\partial C}{\\partial b}  =  \\frac{\\partial C_0}{\\partial b} $$\n",
    "\n",
    "- The bias stays unchanged, while the weight is scaled, which is called **weight decay**:\n",
    "\n",
    "$$ \n",
    "w \\rightarrow w - \\eta \\frac{\\partial C_0}{\\partial w} - \\frac{\\eta \\lambda}{n} w \\\\ \n",
    "=  \\left(1 - \\frac{\\eta \\lambda}{n}\\right) w - \\eta \\frac{\\partial C_0}{\\partial w}\n",
    "$$\n",
    "\n",
    "- Increase the the *regularization parameter* $\\lambda$ by the same factor as the increase in the training data to keep **weight decay** the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization:\n",
    "- The sum of the absolute values of the weights:\n",
    "\n",
    "$$ C = C_0 + \\frac{\\lambda}{n} \\sum_w |w| $$\n",
    "\n",
    "- Penalizes large weights by shrinking the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout:\n",
    "- Instead of midifying the *Cost function*, **Dropout** modifies the Network\n",
    "- Removes some nodes from the Hidden Layer while training\n",
    "- When Tesing, weights should be scaled by multipling by (1-p)% dropout rate to account for Training Dropout\n",
    "- Similar in principle to using ensembles where the effects of different networks is averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Training Data:\n",
    "- Artificially expand the training data by manupulating the data: rotate image, add background noise to sound data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters:\n",
    "- Start with a simpler model and build up later\n",
    "- Use *Grid Search* to searche through a grid in hyper-parameter space\n",
    "- Monitor the validation accuracy more often as the Network is learning\n",
    "- When choosing *hyper-parameters* use *early stopping* loosely to terminate if the best classification accuracy doesn't improve **only** for some time to see the effect of the chosen hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate $\\eta$:\n",
    "- *Stochastic gradient descent*: step gradually down into a valley of the cost function without overshooting the minimum\n",
    "- Start with $\\eta$ that decreases the *Cost function* in the first few epochs, then increase until the *Cost* starts to oscillate or increase: $\\eta = 0.01 \\rightarrow 0.1 \\rightarrow 1.0...$\n",
    "- Decrease $\\eta$ if the initial value causes the *Cost function* to increase $\\eta = 1.0 \\rightarrow 0.1 \\rightarrow 0.01...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule:\n",
    "- It is advantageous to vary the *learning rate*: start out large to change weights quickly and decrease as the Network learns to make more fine adjustments\n",
    "- Decrease the *learning rate* as validation accuracy starts to get worse.  Decrease by a factor of 10 until a certain factor (ex. 1,000) is reached\n",
    "- Start out with a constant *learning rate* and once the Network is optimized, select a *learnng rate schedule* to optimize the Network even further\n",
    "- **Adagrad** provides adaptive learning rate by incorporating knowledge of the geometry of past observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Parameter $\\lambda$:\n",
    "- Strat with $\\lambda = 0$ which eliminates the regularization, then use the validation data to select a good value for $\\lambda$ increaseing or decreasing by a factor of 10\n",
    "- Once $\\lambda$ is picked, adjust as the *Training Data* is adjusted by the same factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization - Adaptive Moment Estimation:\n",
    "- **Adam** combines the advantages of two other extensions of stochastic gradient descent\n",
    "    - Adaptive Gradient Algorithm (AdaGrad) \n",
    "    - Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates\n",
    "- Adam realizes the benefits of both AdaGrad and RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
