{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning\n",
    "- Represent random vector $X$ whose components $x_{i}$ are random variables and quantifiable outcome as random vector $Y$ with joint probability $P(X, Y)$\n",
    "- The joint density of the random variables $x_{i}$ : $f(X)$ and their joint distribution $F(X)$\n",
    "$$f(X) = f(x_{1} , ... . , x_{n})$$ and $$F(X) = F(x_{1} , ... x_{n})$$\n",
    "- Start with empirical distribution $F(X)$ that has empirical parameters $\\theta$ and major objective of statistics is to give exact description of the example based on estimates of the parameters\n",
    "- Predict $x$ vs. Estimate $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Machine Learning\n",
    "- Given some inputs, as representation, calculate something about them \n",
    "- Assume that there is a function $f(X)$ that describes the approximate relationship between $Y$ and $X$:\n",
    "$$f(X) = E(Y|X = x, \\theta)$$\n",
    "where $\\theta$ is parameter of the data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation\n",
    "- Point Estimation is the attempt to provide “best” prediction of some quantity of interest.\n",
    "- Distinguish estimates of parameters from their true values by denoting estimate of a parameter $\\theta$ by $\\hat{\\theta}$ which is also a random variable (function of $x_{i}$ r.v.)\n",
    "- Let ${x^1, x^2, ..., x^n}$ be a set of $n$ independent and identically distributed data points. A **point estimator** or **statistic** is any function of the data:\n",
    "$$\\hat{\\theta}_{n} = g(X) = g(x^1 , x^2 , ..., x^n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation Example\n",
    "- If the function $g(X)$ is properly selected than the estimation error $\\theta - \\hat{\\theta}_{n}$ decreases as number $n$ of examples increases\n",
    "- Assume $\\mu$ denotes the mean grade point average of all college students, and the sample space is {1,2,3,4}. If $x_i$ are the observed grades of a sample of 88 students, then:\n",
    "$$\\hat{\\mu} = \\frac{1}{88} \\sum^{88}_{1} x_{i} = 3.12$$\n",
    "is a point estimate of $\\mu$, the mean grade point average of all the students in the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Estimation\n",
    "- Approximating $f$ with a model or estimate $\\hat{f}$ chosen from hypothesis space\n",
    "- Select functions from a carefully specified set, known as hypothesis space\n",
    "- Decide how to represent data set and select hypothesis space\n",
    "- Generally this space is indexed by a set of parameters $\\theta$ that can be tuned to create different machines:\n",
    "$$H ∶ {f(Y|X, \\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE\n",
    "- Goal is to find **“Maximum Likelihood Estimation”** for parameter $\\theta$.  Find optimal way to fit a distribution to the data.\n",
    "- Consider a set of samples $X$ chosen according to one of family of probability distributions but we don’t know parameters of distribution. We define Likelihood function $$L(\\theta | X) = f(X;θ) = P(X) = x$$ as joined probability distribution of samples X.\n",
    "- Let $P_{model}(X, \\theta)$ be family of probability distributions over the space $\\theta$. The maximum likelihood estimator for $\\theta$ is defined as:\n",
    "$$\\theta_{ML} = arg_{\\theta}max L(\\theta|X)$$\n",
    "$$\\theta_{ML} = arg_{\\theta}max P_{model}(x^1,x^2,...,x^m; \\theta)$$\n",
    "$$\\theta_{ML} = arg_{\\theta}max \\Pi_{i=1}^m P_{model}(x^i; \\theta)$$\n",
    "\n",
    "## MLE Binomial Example:\n",
    "- To solve for the Likelihood, find the derivative of the distribution function\n",
    "- Bernoulli distribution formula is: $P_{model}(x^i; \\theta) = \\theta^{x_{i}}(1 − \\theta)^{1−x_{i}}$\n",
    "- $ L(\\theta|x) = \\Pi_{i=1}^m P_{model}(x^i; \\theta)$\n",
    "- If $Sum = \\sum^{n}_{i=1} x_i$, then $L(\\theta|x) = \\theta^{sum}(1 − \\theta)^{n−sum}$\n",
    "- Find $\\theta$ which maximizes $L(\\theta | x)$ by solving first derivative of $L()$ equal to 0\n",
    "- Solution is: $$\\hat{\\theta}_{ML} = \\frac{\\sum^{n}_{i=1} x_i}{n}$$\n",
    "\n",
    "- For $X$ = {$1,0,0,0,1,1,0,0,0,1$}, $\\theta =4/10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Normal Distribution Example:\n",
    "- Formula for Normal Distribution:\n",
    "$$p(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}$$\n",
    "- $\\mu$ determines the distribution's **Mean**\n",
    "- $\\sigma$ determines **Standard Deviation**\n",
    "- Use likelihood of Normal Distribution to find optimal values for $\\mu$ and $\\sigma$ given data $x$:\n",
    "$$L(\\mu, \\sigma | X) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}$$\n",
    "$$L(\\mu, \\sigma | X) = L(\\mu, \\sigma | x_1, x_2,...,x_n) = L(\\mu, \\sigma | x_1) \\times L(\\mu, \\sigma | x_2) \\times...\\times L(\\mu, \\sigma | x_n)$$\n",
    "- To solve for the maximum likelihood of a parameter ($\\mu$, $\\sigma$), find where the slope of its likelihood functiio is 0\n",
    "- Take 2 different derivatives:\n",
    "    - One with respect to $\\mu$ when $\\sigma$ is constant, to find MLE where derivative is 0\n",
    "    - One with respect ro $\\sigma$ when $\\mu$ is constant, to find MLE where derivative is 0\n",
    "1. Take the log of both functions (Log of likelihood function peaks at the same value)\n",
    "2. $\\ln(L(\\mu, \\sigma | X)) = \\ln(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-(x_1-\\mu)^2/2\\sigma^2}) \\times...\\times \\ln(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-(x_n-\\mu)^2/2\\sigma^2})$\n",
    "- The derivative of the Log function with rspect to $\\mu$ is:\n",
    "$$\\frac{\\partial}{{\\partial\\mu}} \\ln(L(\\mu, \\sigma | X)) = \\frac{1}{\\sigma^2}((x_1+x_2+...+x_n) - n\\mu)$$\n",
    "- Solve for 0\n",
    "$$\\frac{1}{\\sigma^2}((x_1+x_2+...+x_n) - n\\mu) = 0$$\n",
    "$$\\mu = \\frac{(x_1+x_2+...+x_n)}{n}$$ \n",
    "- MLE for $\\mu$ is the **Mean** of the measurements \n",
    "- The derivative of the Log function with rspect to $\\sigma$ is:\n",
    "$$\\frac{\\partial}{{\\partial\\sigma}} \\ln(L(\\mu, \\sigma | X)) = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}((x_1-\\mu)^2 + ... + (x_n - \\mu)^2)$$\n",
    "- Solve for 0\n",
    "$$-\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}((x_1-\\mu)^2 + ... + (x_n - \\mu)^2) = 0$$\n",
    "$$\\sigma = \\sqrt{\\frac{(x_1-\\mu)^2+...+(x_n - \\mu)^2}{n}}$$\n",
    "- MLE for $\\sigma$ is the **Standard Deviation** of the Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy \n",
    "- Entropy is a measure of uncertainty to the occurrence or nonoccurrence of any event from a collection of multiple mutually exclusive events\n",
    "- For discrete random variable $X$ with $K$ states and pmf is probability $P[x=k]$\n",
    "- Quantify uncertainty of event $x_k = {x=k}$.  The uncertainty will be maximized if $p(x = k) = 1/K$\n",
    "- Hence, measure of uncertainty satisfies property: $I(x=k) = −\\ln P[x = k]$\n",
    "- Define “Information for an event”: $I(x) = −\\ln P(x)$, it is measured in bits or shannons\n",
    "- The entropy of a random variable $X$ with distribution $P$ is defined as the expected value of the uncertainty of its outcomes:\n",
    "$$H(X) = E[I(x)] = −\\sum^{K}_{k=1} P[x=k] \\ln P[x=k]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Entropy\n",
    "- For special case of binary random variable $X$ {$0,1$}\n",
    "$$p(X=1) = \\theta \\space and \\space p(X=0) = 1 − \\theta$$\n",
    "$$H(X) = −[p(X = 1) \\log_2 p(X=1) + p(X=0) \\log_2 p(X = 0)]$$\n",
    "$$H(X) = −[\\theta \\log_2 \\theta + (1 − \\theta)\\log_2 (1 − \\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "- Measure how different two distributions $P(x)$ and $Q(x)$ are using **Kullback-Leibler (KL) divergence**:\n",
    "$$D_{KL} (P||Q) = E_{x \\sim P}[\\log \\frac{P(x)}{Q(x)}]$$\n",
    "$$ D_{KL} (P||Q) = E_{x \\sim P}[\\log P(x) - \\log Q(x)]$$\n",
    "- The KL divergence has many useful properties:\n",
    "    - Nonnegative\n",
    "    - Not symmetric\n",
    "- KL is not a true distance measure between two distributions because it is asymmetric. The choice which direction to use for KL is problem dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "- Cross Entropy between two distributions $P$ and $Q$\n",
    "$$H(P,Q) = H(P) + D_{KL} (P||Q)$$\n",
    "$$H(P,Q) = − E_{x \\sim P} [\\log Q(x)]$$\n",
    "- Cross Entropy is always larger than Entropy, and is equal only when $P$ and $Q$ are the same\n",
    "- Minimizing Cross Entropy will move closer to the desired distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Activations and Common Activation Functions\n",
    "- In artificial neurons, use activation function to propagate the output of one layer’s nodes forward to next layer\n",
    "- Activation functions, model non-linearity in NN that cannot be solved with linear functions (scalar to scalar functions)\n",
    "- Functions used in Deep Learning models after linear combination of inputs; mimic neuron firing \n",
    "- Used to calculate parameters of the probability distributions used in probabilistic DL models\n",
    "- Simplest Linear function: $f(x) = wx$\n",
    "    - Function passes the signal through unchanged\n",
    "    - Mostly used in the input layer of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit - ReLU\n",
    "- Activates a node only if the input is above a certain threshold:\n",
    "$$f(x) = max(0,kx)$$\n",
    "- In this example, the threshold is 0, and x is the input to the neuron\n",
    "- ReLU is very common in the current state of art DNN\n",
    "- Popular modification in MobileNet is ReLU6 which behaves as general ReLU, but output saturates at level 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Sigmoid\n",
    "- Logistic Sigmoid Function:\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- Saturates when its argument is very positive or very negative, it becomes **insensitive** to small changes in the input\n",
    "- Used for binary classification class \n",
    "- Commonly used to produce $p$ parameter of Bernoulli distribution (between 0 and 1): \n",
    "$$p(y=1|x;\\theta) = \\sigma(\\theta^T x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "- Softmax function, generalization of logistic function, (soft version of a maximum) is:\n",
    "$$ g(x_i) = \\frac{e^{-x^i}}{\\sum_{j=1}^k e^{xj}}, i=1,2...,k $$\n",
    "- Useful property is that calculated probabilities are in range $(0,1)$, but sum of all $P$ is equal to 1 \n",
    "- High value will have high probability\n",
    "- The squashing function, its output always sums to 1, “winner-take-all” model \n",
    "- Used for multiple classification logistic regression model\n",
    "- Works well with MLE used as estimator when model is learning parameters\n",
    "- It has multiple output values that can saturate $$softmax(z) = softmax(z + c)$$ $c$ is some scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
