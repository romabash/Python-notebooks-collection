{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Receptive Fields:\n",
    "- Instead of 1D input vector, inputs in convolutional net are 2D\n",
    "- Instead of being Fully connected as in FCNN, small localized regions are connected in CNN\n",
    "- 5x5 region can be connected to a neuron with 25 inputs, each with a learned weight and a neuron bias \n",
    "- A 28x28 input layer with a 5x5 **local receptive field** and *stride length* of 1 results in a 24x24 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Weights and Biases:\n",
    "- Same weights and bias are used for each of the hidden neurons connected to the **local receptive fields**\n",
    "- All the neurons in the hidden layer detect the same feature in different location\n",
    "- Shared weights and bias define a **kernel** or **filter**\n",
    "- A map from input to hidden layer is refered to as *feature map*\n",
    "- Each *feature map* recognize one feature (at different locations in the image) and to recognize several different features, convolutional layers consists of several different *feature maps*\n",
    "- Advantage of sharing weights and biases is that it greatly reduces the number of parameters involved in a convolutional network.  For each 5x5 *feature map* and 1 bias, there are only 26 parameters.  Much less than in a FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers:\n",
    "- Condense and simplify the information in the output from the convolutional hidden layer and reduces the number of parameters\n",
    "- A 2x2 *max-pooling* layer outputs the maximum activation in the 2×2 input region of the convolutional hidden layer\n",
    "- **Pooling Layers** have no learning Parameters and *Hyperparameters* for Filter Size and Stride\n",
    "- Output Shape after applying **Pooling Layer**:\n",
    "$$ \\lfloor \\frac{n+2p-f}{s}+1 \\rfloor $$\n",
    "- 24x24 convolutional hidden layer with 2x2 **pooling layer** with **stride** of 1, results in 12x12 neurons\n",
    "- Apply *max-pooling* to each feature map separately:  A 3x24x24 convolution hidden layers, result in 3X12x12 max-pooling layers.  The *Volume* or number of channels stays the same\n",
    "- Other Pooling methods include *average-pooling* and *L2 pooling* where the square root of the sum of the squares of the activations in the 2×2 region is taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example CNN:\n",
    "- **28x28 input layer (corresponding to image pixels)**\n",
    "    - Convolutional Layer with 5×5 *local receptive field* and 3 feature maps: 3x5x5\n",
    "        - **3×24×24 Hidden feature neurons layer**\n",
    "            - *Max-Pooling* Layer with 2×2 regions across 3 feature maps: 3x2x2\n",
    "                -  **3×12×12 Hidden feature neurons**\n",
    "                    - FCNN, connects every neuron from the 3x12x12 *max-pooled* layer to the 100 neurons hidden layer...\n",
    "- The convolutional and pooling layers learn local spatial structures, while the fully-connected layers learn  more abstract information from across the entire image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding:\n",
    "- Hidden Layer output after applying a $f \\times f$ *feature map* to $n \\times n$ *Input*: $(n-f+1) \\times (n-f+1)$\n",
    "- Apply **Padding** to avoid shrinking output and maximuze input information at the *edges*\n",
    "- With Padding the *Output Shape* is: $(n+2p-f+1) \\times (n+2p-f+1)$\n",
    "- **Valid** Convolution: No padding, output shape is smaller than the input shape\n",
    "- **Same** Convolution: Padding where output shape is equals to the input shape\n",
    "    - Choose odd number of filters\n",
    "    - Choice for padding $p$ is: $ p = \\frac{f-1}{2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride:\n",
    "- Formula for Output Shape when using $n \\times n$ Input, $f \\times f$ Filter, $p$ Padding, and $s$ Stride: $$\\lfloor \\frac{n+2p-f}{s}+1 \\rfloor \\times \\lfloor\\frac{n+2p-f}{s}+1 \\rfloor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Over Volume (RGB Images):\n",
    "- Allows to detect features in either one channel or all channels at the same time, depending on the parameters in the *Filter* channels\n",
    "- Input Shape of $n_h \\times n_w \\times n_c$ with Filter $f_h \\times f_w \\times f_c$, where number of channels $f_c$ in Filter have to match number of channels $n_c$ in Input\n",
    "- The Output Shape of each Filter Map is a *Flat* Matirx with results of each Convolution over the Volume added together\n",
    "$$(n_h \\times n_w \\times n_c) \\times (n_h \\times n_w \\times n_c) * (f_h \\times f_w \\times f_c) \\times (f_h \\times f_w \\times f_c) \\rightarrow (n-f+1) \\times (n-f+1) \\times n_c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer:\n",
    "$$ a^{[l]} = g(z^{[l]}) $$\n",
    "where:\n",
    "$$ z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]} $$\n",
    "where: <br>\n",
    "<div style=\"text-align: center\">\n",
    "    $a^{[l-1]}$ is <em>Activation</em> of Input <br>\n",
    "    $w^{[l]}$ is <em>Weights</em> of the Filter Map <br>\n",
    "    $b^{[l]}$ is <em>Bias</em> of the Output\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "- **Input**: $n_h^{[l-1]} \\times n_w^{[l-1]} \\times n_c^{[l-1]}$\n",
    "- **Filter Size**: $f^{[l]}$\n",
    "    - $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]}$, where $n_c^{[l-1]}$ matches the number of channels in the Input\n",
    "- **Padding**: $p^{[l]}$\n",
    "- **Stride**: $s^{[l]}$\n",
    "- **Output**: $n_h^{[l]} \\times n_w^{[l]} \\times n_c^{[l]}$\n",
    "<br><br>\n",
    "where\n",
    "$$ n_h^{[l]} = \\lfloor \\frac{n_h^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \\rfloor$$\n",
    "$$ n_w^{[l]} = \\lfloor \\frac{n_h^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \\rfloor$$\n",
    "<div style=\"text-align: center\"> and $n_c^{[l]}$ is number of filters </div>\n",
    "<br>\n",
    "- **Activation**: $a^{[l]} \\rightarrow n_h^{[l]} \\times n_w^{[l]} \\times n_c^{[l]}$\n",
    "- **Weights**: $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} \\times n_c^{[l]}$, where $n_c^{[l]}$ is the number of filters in Layer $l$\n",
    "- **Bias**: $n_c^{[l]}$, same as number of filters in Layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Networks:\n",
    "- LeNet-5: \n",
    "    - Convolution/Average Pooling Layers that reduce the size of input at each level\n",
    "- AlexNet:\n",
    "    - Much bigger than *LeNet-5* Network\n",
    "    - First Conv Layer used 96 Filters with Stride of 4 to reduce dimension from 227x227x3 to 55x55x96\n",
    "    - Uses Max Pooling with **same** padding in the network\n",
    "    - Uses **LeRu** as an activation function, and **Softmax** as an Output Layer\n",
    "- VGG-16:\n",
    "    - Uses simpler Network with similar Layers:\n",
    "        - Conv Filters = 3x3 with stride = 1 and padding = same\n",
    "        - Max-Pool = 2x2 with stride = 2\n",
    "    - Convolution layer with *same* padding keeps the dimensions the same, while the Pooling layers reduce the dimension at each Level\n",
    "    - Uniformed Architecture where *height* dimension reduced roughly by half while *width* dimension increased roughly twice at each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNets: Residual Networks\n",
    "- Adding a shortcut/skipping a Layer: $a^{[l]} \\rightarrow a^{[l+2]}$ \n",
    "- Easy to learn the *identity function* when $a^{[l+2]}$ goes to zero \n",
    "    - $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$\n",
    "    - $a^{[l+2]} = g(w^{[l+2]} \\cdot a^{[l+1]} + b^{[l+2]} + a^{[l]})$\n",
    "- Deeper Networks don't slow down as $w^{[l+2]}$ or $b^{[l+2]}$  approach 0, since $a^{[l+2]}$ is just $a^{[l]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Networks:\n",
    "- Use several *types* of layers (Convolution or Pooling with same padding) and combine (stack) the output into a single $n_h \\times n_w \\times n_c$ Output\n",
    "    - Can use 1x1 Convolutions to reduce the number of channels while keeping the height and width dimensions the same using $1 \\times 1 \\times n_c$ filters.  Also reduces computational power needed as compared to using $n \\times n$ filters with *same* padding\n",
    "    - Can use $n \\times n$ filters with **same** padding to produce an Output of the same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
